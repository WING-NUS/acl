<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.cl">
  <volume id="1">
    <meta>
      <booktitle>Computational Linguistics, Volume 46, Issue 1 - March 2020</booktitle>
      <month>March</month>
      <year>2020</year>
    </meta>

    <paper id="1">
      <title>On the Linguistic Representational Power of Neural Machine Translation Models</title>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <author><first>Nadir</first><last>Durrani</last></author>
      <author><first>Fahim</first><last>Dalvi</last></author>
      <author><first>Hassan</first><last>Sajjad</last></author>
      <author><first>James</first><last>Glass</last></author>
      <doi>10.1162/coli_a_00367</doi>
      <abstract>Despite the recent success of deep neural networks in natural language processing and other spheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation (NMT) models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word structure captured within the learned representations, which is an important aspect in translating morphologically rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models trained in an end-to-end fashion, without being provided any direct supervision during the training process, learn a non-trivial amount of linguistic information. Notable findings include the following observations: (i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers of the model; (iii) Representations learned using characters are more informed about word-morphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.</abstract>
      <pages>1–52</pages>
      <url hash="72e17aed">2020.cl-1.1</url>
    </paper>

    <paper id="2">
      <title>The Design and Implementation of XiaoIce, an Empathetic Social Chatbot</title>
      <author><first>Li</first><last>Zhou</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <author><first>Di</first><last>Li</last></author>
      <author><first>Heung-Yeung</first><last>Shum</last></author>
      <doi>10.1162/coli_a_00368</doi>
      <abstract>This article describes the development of Microsoft XiaoIce, the most popular social chatbot in the world. XiaoIce is uniquely designed as an artifical intelligence companion with an emotional connection to satisfy the human need for communication, affection, and social belonging. We take into account both intelligent quotient and emotional quotient in system design, cast human–machine social chat as decision-making over Markov Decision Processes, and optimize XiaoIce for long-term user engagement, measured in expected Conversation-turns Per Session (CPS). We detail the system architecture and key components, including dialogue manager, core chat, skills, and an empathetic computing module. We show how XiaoIce dynamically recognizes human feelings and states, understands user intent, and responds to user needs throughout long conversations. Since the release in 2014, XiaoIce has communicated with over 660 million active users and succeeded in establishing long-term relationships with many of them. Analysis of large-scale online logs shows that XiaoIce has achieved an average CPS of 23, which is significantly higher than that of other chatbots and even human conversations.</abstract>
      <pages>53–93</pages>
      <url hash="5be02ff3">2020.cl-1.2</url>
    </paper>

    <paper id="3">
      <title>An Empirical Study on Crosslingual Transfer in Probabilistic Topic Models</title>
      <author><first>Shudong</first><last>Hao</last></author>
      <author><first>Michael
                            J.</first><last>Paul</last></author>
      <doi>10.1162/coli_a_00369</doi>
      <abstract>Probabilistic topic modeling is a common first step in crosslingual tasks to enable knowledge transfer and extract multilingual features. Although many multilingual topic models have been developed, their assumptions about the training corpus are quite varied, and it is not clear how well the different models can be utilized under various training conditions. In this article, the knowledge transfer mechanisms behind different multilingual topic models are systematically studied, and through a broad set of experiments with four models on ten languages, we provide empirical insights that can inform the selection and future development of multilingual topic models.</abstract>
      <pages>95–134</pages>
      <url hash="1a2d5806">2020.cl-1.3</url>
    </paper>

    <paper id="4">
      <title>Data-Driven Sentence Simplification: Survey and Benchmark</title>
      <author><first>Fernando</first><last>Alva-Manchego</last></author>
      <author><first>Carolina</first><last>Scarton</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <doi>10.1162/coli_a_00370</doi>
      <abstract>Sentence Simplification (SS) aims to modify a sentence in order to make it easier to read and understand. In order to do so, several rewriting transformations can be performed such as replacement, reordering, and splitting. Executing these transformations while keeping sentences grammatical, preserving their main idea, and generating simpler output, is a challenging and still far from solved problem. In this article, we survey research on SS, focusing on approaches that attempt to learn how to simplify using corpora of aligned original-simplified sentence pairs in English, which is the dominant paradigm nowadays. We also include a benchmark of different approaches on common data sets so as to compare them and highlight their strengths and limitations. We expect that this survey will serve as a starting point for researchers interested in the task and help spark new ideas for future developments.</abstract>
      <pages>135–187</pages>
      <url hash="dc68839b">2020.cl-1.4</url>
    </paper>

    <paper id="5">
      <title>Corpora Annotated with Negation: An Overview</title>
      <author><first>Salud María</first><last>Jiménez-Zafra</last></author>
      <author><first>Roser</first><last>Morante</last></author>
      <author><first>María Teresa</first><last>Martín-Valdivia</last></author>
      <author><first>L. Alfonso</first><last>Ureña-López</last></author>
      <doi>10.1162/coli_a_00371</doi>
      <abstract>Negation is a universal linguistic phenomenon with a great qualitative impact on natural language processing applications. The availability of corpora annotated with negation is essential to training negation processing systems. Currently, most corpora have been annotated for English, but the presence of languages other than English on the Internet, such as Chinese or Spanish, is greater every day. In this study, we present a review of the corpora annotated with negation information in several languages with the goal of evaluating what aspects of negation have been annotated and how compatible the corpora are. We conclude that it is very difficult to merge the existing corpora because we found differences in the annotation schemes used, and most importantly, in the annotation guidelines: the way in which each corpus was tokenized and the negation elements that have been annotated. Differently than for other well established tasks like semantic role labeling or parsing, for negation there is no standard annotation scheme nor guidelines, which hampers progress in its treatment.</abstract>
      <pages>1–52</pages>
      <url hash="64d1b623">2020.cl-1.5</url>
    </paper>
  </volume>



  <volume id="2">
    <meta>
      <booktitle>Computational Linguistics, Volume 46, Issue 2 - June 2020</booktitle>
      <month>June</month>
      <year>2020</year>
    </meta>

    <paper id="1">
      <title>Multilingual and Interlingual Semantic Representations for Natural Language Processing: A Brief Introduction</title>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <author><first>Cristina</first><last>España-Bonet</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <doi>10.1162/coli_a_00373</doi>
      <abstract>We introduce the Computational Linguistics special issue on Multilingual and Interlingual Semantic Representations for Natural Language Processing. We situate the special issue’s five articles in the context of our fast-changing field, explaining our motivation for this project. We offer a brief summary of the work in the issue, which includes developments on lexical and sentential semantic representations, from symbolic and neural perspectives.</abstract>
      <pages>249–255</pages>
      <url hash="1ea1e59e">2020.cl-2.1</url>
    </paper>

    <paper id="2">
      <title>Unsupervised Word Translation with Adversarial Autoencoder</title>
      <author><first>Tasnim</first><last>Mohiuddin</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <doi>10.1162/coli_a_00374</doi>
      <abstract>Crosslingual word embeddings learned from monolingual embeddings have a crucial role in many downstream tasks, ranging from machine translation to transfer learning. Adversarial training has shown impressive success in learning crosslingual embeddings and the associated word translation task without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this article, we investigate adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. We use two types of refinement procedures sequentially after obtaining the trained encoders and mappings from the adversarial training, namely, refinement with Procrustes solution and refinement with symmetric re-weighting. Extensive experimentations with high- and low-resource languages from two different data sets show that our method achieves better performance than existing adversarial and non-adversarial approaches and is also competitive with the supervised system. Along with performing comprehensive ablation studies to understand the contribution of different components of our adversarial model, we also conduct a thorough analysis of the refinement procedures to understand their effects.</abstract>
      <pages>257–288</pages>
      <url hash="74f783bb">2020.cl-2.2</url>
    </paper>

    <paper id="3">
      <title>LessLex: Linking Multilingual Embeddings to SenSe Representations of LEXical Items</title>
      <author><first>Davide</first><last>Colla</last></author>
      <author><first>Enrico</first><last>Mensa</last></author>
      <author><first>Daniele P.</first><last>Radicioni</last></author>
      <doi>10.1162/coli_a_00375</doi>
      <abstract>We present LESSLEX, a novel multilingual lexical resource. Different from the vast majority of existing approaches, we ground our embeddings on a sense inventory made available from the BabelNet semantic network. In this setting, multilingual access is governed by the mapping of terms onto their underlying sense descriptions, such that all vectors co-exist in the same semantic space. As a result, for each term we have thus the “blended” terminological vector along with those describing all senses associated to that term. LESSLEX has been tested on three tasks relevant to lexical semantics: conceptual similarity, contextual similarity, and semantic text similarity. We experimented over the principal data sets for such tasks in their multilingual and crosslingual variants, improving on or closely approaching state-of-the-art results. We conclude by arguing that LESSLEX vectors may be relevant for practical applications and for research on conceptual and lexical access and competence.</abstract>
      <pages>289–333</pages>
      <url hash="69547672">2020.cl-2.3</url>
    </paper>

    <paper id="4">
      <title>LINSPECTOR: Multilingual Probing Tasks for Word Representations</title>
      <author><first>Gözde Gül</first><last>Şahin</last></author>
      <author><first>Clara</first><last>Vania</last></author>
      <author><first>Ilia</first><last>Kuznetsov</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <doi>10.1162/coli_a_00376</doi>
      <abstract>Despite an ever-growing number of word representation models introduced for a large number of languages, there is a lack of a standardized technique to provide insights into what is captured by these models. Such insights would help the community to get an estimate of the downstream task performance, as well as to design more informed neural architectures, while avoiding extensive experimentation that requires substantial computational resources not all researchers have access to. A recent development in NLP is to use simple classification tasks, also called probing tasks, that test for a single linguistic feature such as part-of-speech. Existing studies mostly focus on exploring the linguistic information encoded by the continuous representations of English text. However, from a typological perspective the morphologically poor English is rather an outlier: The information encoded by the word order and function words in English is often stored on a subword, morphological level in other languages. To address this, we introduce 15 type-level probing tasks such as case marking, possession, word length, morphological tag count, and pseudoword identification for 24 languages. We present a reusable methodology for creation and evaluation of such tests in a multilingual setting, which is challenging because of a lack of resources, lower quality of tools, and differences among languages. We then present experiments on several diverse multilingual word embedding models, in which we relate the probing task performance for a diverse set of languages to a range of five classic NLP tasks: POS-tagging, dependency parsing, semantic role labeling, named entity recognition, and natural language inference. We find that a number of probing tests have significantly high positive correlation to the downstream tasks, especially for morphologically rich languages. We show that our tests can be used to explore word embeddings or black-box neural models for linguistic cues in a multilingual setting. We release the probing data sets and the evaluation suite LINSPECTOR with https://github.com/UKPLab/linspector.</abstract>
      <pages>335–385</pages>
      <url hash="bcb62861">2020.cl-2.4</url>
    </paper>

    <paper id="5">
      <title>A Systematic Study of Inner-Attention-Based Sentence Representations in Multilingual Neural Machine Translation</title>
      <author><first>Raúl</first><last>Vázquez</last></author>
      <author><first>Alessandro</first><last>Raganato</last></author>
      <author><first>Mathias</first><last>Creutz</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <doi>10.1162/coli_a_00377</doi>
      <abstract>Neural machine translation has considerably improved the quality of automatic translations by learning good representations of input sentences. In this article, we explore a multilingual translation model capable of producing fixed-size sentence representations by incorporating an intermediate crosslingual shared layer, which we refer to as attention bridge. This layer exploits the semantics from each language and develops into a language-agnostic meaning representation that can be efficiently used for transfer learning. We systematically study the impact of the size of the attention bridge and the effect of including additional languages in the model. In contrast to related previous work, we demonstrate that there is no conflict between translation performance and the use of sentence representations in downstream tasks. In particular, we show that larger intermediate layers not only improve translation quality, especially for long sentences, but also push the accuracy of trainable classification tasks. Nevertheless, shorter representations lead to increased compression that is beneficial in non-trainable similarity tasks. Similarly, we show that trainable downstream tasks benefit from multilingual models, whereas additional language signals do not improve performance in non-trainable benchmarks. This is an important insight that helps to properly design models for specific applications. Finally, we also include an in-depth analysis of the proposed attention bridge and its ability to encode linguistic properties. We carefully analyze the information that is captured by individual attention heads and identify interesting patterns that explain the performance of specific settings in linguistic probing tasks.</abstract>
      <pages>387–424</pages>
      <url hash="a8a898e3">2020.cl-2.5</url>
    </paper>

    <paper id="6">
      <title>Abstract Syntax as Interlingua: Scaling Up the Grammatical Framework from Controlled Languages to Robust Pipelines</title>
      <author><first>Aarne</first><last>Ranta</last></author>
      <author><first>Krasimir</first><last>Angelov</last></author>
      <author><first>Normunds</first><last>Gruzitis</last></author>
      <author><first>Prasanth</first><last>Kolachina</last></author>
      <doi>10.1162/coli_a_00378</doi>
      <abstract>Abstract syntax is an interlingual representation used in compilers. Grammatical Framework (GF) applies the abstract syntax idea to natural languages. The development of GF started in 1998, first as a tool for controlled language implementations, where it has gained an established position in both academic and commercial projects. GF provides grammar resources for over 40 languages, enabling accurate generation and translation, as well as grammar engineering tools and components for mobile and Web applications. On the research side, the focus in the last ten years has been on scaling up GF to wide-coverage language processing. The concept of abstract syntax offers a unified view on many other approaches: Universal Dependencies, WordNets, FrameNets, Construction Grammars, and Abstract Meaning Representations. This makes it possible for GF to utilize data from the other approaches and to build robust pipelines. In return, GF can contribute to data-driven approaches by methods to transfer resources from one language to others, to augment data by rule-based generation, to check the consistency of hand-annotated corpora, and to pipe analyses into high-precision semantic back ends. This article gives an overview of the use of abstract syntax as interlingua through both established and emerging NLP applications involving GF.</abstract>
      <pages>425–486</pages>
      <url hash="ad744fa7">2020.cl-2.6</url>
    </paper>

    <paper id="7">
      <title>Fair Is Better than Sensational: Man Is to Doctor as Woman Is to Doctor</title>
      <author><first>Malvina</first><last>Nissim</last></author>
      <author><first>Rik</first><last>van Noord</last></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <doi>10.1162/coli_a_00379</doi>
      <abstract>Analogies such as man is to king as woman is to X are often used to illustrate the amazing power of word embeddings. Concurrently, they have also been used to expose how strongly human biases are encoded in vector spaces trained on natural language, with examples like man is to computer programmer as woman is to homemaker. Recent work has shown that analogies are in fact not an accurate diagnostic for bias, but this does not mean that they are not used anymore, or that their legacy is fading. Instead of focusing on the intrinsic problems of the analogy task as a bias detection tool, we discuss a series of issues involving implementation as well as subjective choices that might have yielded a distorted picture of bias in word embeddings. We stand by the truth that human biases are present in word embeddings, and, of course, the need to address them. But analogies are not an accurate tool to do so, and the way they have been most often used has exacerbated some possibly non-existing biases and perhaps hidden others. Because they are still widely popular, and some of them have become classics within and outside the NLP community, we deem it important to provide a series of clarifications that should put well-known, and potentially new analogies, into the right perspective.</abstract>
      <pages>487–497</pages>
      <url hash="e64e92e7">2020.cl-2.7</url>
    </paper>

    <paper id="8">
      <title>The Limitations of Stylometry for Detecting Machine-Generated Fake News</title>
      <author><first>Tal</first><last>Schuster</last></author>
      <author><first>Roei</first><last>Schuster</last></author>
      <author><first>Darsh J.</first><last>Shah</last></author>
      <author><first>Regina</first><last>Barzilay</last></author>
      <doi>10.1162/coli_a_00380</doi>
      <abstract>Recent developments in neural language models (LMs) have raised concerns about their potential misuse for automatically spreading misinformation. In light of these concerns, several studies have proposed to detect machine-generated fake news by capturing their stylistic differences from human-written text. These approaches, broadly termed stylometry, have found success in source attribution and misinformation detection in human-written texts. However, in this work, we show that stylometry is limited against machine-generated misinformation. Whereas humans speak differently when trying to deceive, LMs generate stylistically consistent text, regardless of underlying motive. Thus, though stylometry can successfully prevent impersonation by identifying text provenance, it fails to distinguish legitimate LM applications from those that introduce false information. We create two benchmarks demonstrating the stylistic similarity between malicious and legitimate uses of LMs, utilized in auto-completion and editing-assistance settings.1 Our findings highlight the need for non-stylometry approaches in detecting machine-generated misinformation, and open up the discussion on the desired evaluation benchmarks.</abstract>
      <pages>499–510</pages>
      <url hash="fb11029c">2020.cl-2.8</url>
    </paper>
  </volume>
</collection>
