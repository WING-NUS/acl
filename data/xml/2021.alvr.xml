<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.alvr">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Advances in Language and Vision Research</booktitle>
      <editor><first/><last>Xin</last></editor>
      <editor><first>Ronghang</first><last>Hu</last></editor>
      <editor><first>Drew</first><last>Hudson</last></editor>
      <editor><first>Tsu-Jui</first><last>Fu</last></editor>
      <editor><first>Marcus</first><last>Rohrbach</last></editor>
      <editor><first>Daniel</first><last>Fried</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.alvr-1</url>
    </meta>
    <frontmatter>
      <url hash="4368e7cb">2021.alvr-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Feature-level Incongruence Reduction for Multimodal Translation</title>
      <author><first>Zhifeng</first><last>Li</last></author>
      <author><first>Yu</first><last>Hong</last></author>
      <author><first>Yuchen</first><last>Pan</last></author>
      <author><first>Jian</first><last>Tang</last></author>
      <author><first>Jianmin</first><last>Yao</last></author>
      <author><first>Guodong</first><last>Zhou</last></author>
      <pages>1–10</pages>
      <abstract>Caption translation aims to translate image annotations (captions for short). Recently, Multimodal Neural Machine Translation (MNMT) has been explored as the essential solution. Besides of linguistic features in captions, MNMT allows visual(image) features to be used. The integration of multimodal features reinforces the semantic representation and considerably improves translation performance. However, MNMT suffers from the incongruence between visual and linguistic features. To overcome the problem, we propose to extend MNMT architecture with a harmonization network, which harmonizes multimodal features(linguistic and visual features)by unidirectional modal space conversion. It enables multimodal translation to be carried out in a seemingly monomodal translation pipeline. We experiment on the golden Multi30k-16 and 17. Experimental results show that, compared to the baseline,the proposed method yields the improvements of 2.2% BLEU for the scenario of translating English captions into German (En→De) at best,7.6% for the case of English-to-French translation(En→Fr) and 1.5% for English-to-Czech(En→Cz). The utilization of harmonization network leads to the competitive performance to the-state-of-the-art.</abstract>
      <url hash="e6bff897">2021.alvr-1.1</url>
    </paper>
    <paper id="2">
      <title>Error Causal inference for Multi-Fusion models</title>
      <author><first>Chengxi</first><last>Li</last></author>
      <author><first>Brent</first><last>Harrison</last></author>
      <pages>11–15</pages>
      <abstract>In this paper, we propose an error causal inference method that could be used for finding dominant features for a faulty instance under a well-trained multi-modality input model, which could apply to any testing instance. We evaluate our method using a well-trained multi-modalities stylish caption generation model and find those causal inferences that could provide us the insights for next step optimization.</abstract>
      <url hash="c7e38187">2021.alvr-1.2</url>
    </paper>
    <paper id="3">
      <title>Leveraging Partial Dependency Trees to Control Image Captions</title>
      <author><first>Wenjie</first><last>Zhong</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <pages>16–21</pages>
      <abstract>Controlling the generation of image captions attracts lots of attention recently. In this paper, we propose a framework leveraging partial syntactic dependency trees as control signals to make image captions include specified words and their syntactic structures. To achieve this purpose, we propose a Syntactic Dependency Structure Aware Model (SDSAM), which explicitly learns to generate the syntactic structures of image captions to include given partial dependency trees. In addition, we come up with a metric to evaluate how many specified words and their syntactic dependencies are included in generated captions. We carry out experiments on two standard datasets: Microsoft COCO and Flickr30k. Empirical results show that image captions generated by our model are effectively controlled in terms of specified words and their syntactic structures.The code is available on GitHub.</abstract>
      <url hash="c9112d4c">2021.alvr-1.3</url>
    </paper>
    <paper id="4">
      <title>Grounding Plural Phrases: Countering Evaluation Biases by Individuation</title>
      <author><first>Julia</first><last>Suter</last></author>
      <author><first>Letitia</first><last>Parcalabescu</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <pages>22–28</pages>
      <abstract>Phrase grounding (PG) is a multimodal task that grounds language in images. PG systems are evaluated on well-known benchmarks, using Intersection over Union (IoU) as evaluation metric. This work highlights a disconcerting bias in the evaluation of grounded plural phrases, which arises from representing sets of objects as a union box covering all component bounding boxes, in conjunction with the IoU metric. We detect, analyze and quantify an evaluation bias in the grounding of plural phrases and define a novel metric, c-IoU, based on a union box’s component boxes. We experimentally show that our new metric greatly alleviates this bias and recommend using it for fairer evaluation of plural phrases in PG tasks.</abstract>
      <url hash="a92f2c3a">2021.alvr-1.4</url>
    </paper>
    <paper id="5">
      <title><fixed-case>P</fixed-case>an<fixed-case>GEA</fixed-case>: The Panoramic Graph Environment Annotation Toolkit</title>
      <author><first>Alexander</first><last>Ku</last></author>
      <author><first>Peter</first><last>Anderson</last></author>
      <author><first>Jordi</first><last>Pont Tuset</last></author>
      <author><first>Jason</first><last>Baldridge</last></author>
      <pages>29–33</pages>
      <abstract>PanGEA, the Panoramic Graph Environment Annotation toolkit, is a lightweight toolkit for collecting speech and text annotations in photo-realistic 3D environments. PanGEA immerses annotators in a web-based simulation and allows them to move around easily as they speak and/or listen. It includes database and cloud storage integration, plus utilities for automatically aligning recorded speech with manual transcriptions and the virtual pose of the annotators. Out of the box, PanGEA supports two tasks – collecting navigation instructions and navigation instruction following – and it could be easily adapted for annotating walking tours, finding and labeling landmarks or objects, and similar tasks. We share best practices learned from using PanGEA in a 20,000 hour annotation effort to collect the Room-Across-Room dataset. We hope that our open-source annotation toolkit and insights will both expedite future data collection efforts and spur innovation on the kinds of grounded language tasks such environments can support.</abstract>
      <url hash="27d10761">2021.alvr-1.5</url>
    </paper>
    <paper id="6">
      <title>Learning to Learn Semantic Factors in Heterogeneous Image Classification</title>
      <author><first>Boyue</first><last>Fan</last></author>
      <author><first>Zhenting</first><last>Liu</last></author>
      <pages>34–38</pages>
      <abstract>Few-shot learning is to recognize novel classes with a few labeled samples per class. Although numerous meta-learning methods have made significant progress, they struggle to directly address the heterogeneity of training and evaluating task distributions, resulting in the domain shift problem when transitioning to new tasks with disjoint spaces. In this paper, we propose a novel method to deal with the heterogeneity. Specifically, by simulating class-difference domain shift during the meta-train phase, a bilevel optimization procedure is applied to learn a transferable representation space that can rapidly adapt to heterogeneous tasks. Experiments demonstrate the effectiveness of our proposed method.</abstract>
      <url hash="0f741cf6">2021.alvr-1.6</url>
    </paper>
    <paper id="7">
      <title>Reference and coreference in situated dialogue</title>
      <author><first>Sharid</first><last>Loáiciga</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>39–44</pages>
      <abstract>In recent years several corpora have been developed for vision and language tasks. We argue that there is still significant room for corpora that increase the complexity of both visual and linguistic domains and which capture different varieties of perceptual and conversational contexts. Working with two corpora approaching this goal, we present a linguistic perspective on some of the challenges in creating and extending resources combining language and vision while preserving continuity with the existing best practices in the area of coreference annotation.</abstract>
      <url hash="0ea1eb50">2021.alvr-1.7</url>
    </paper>
    <paper id="8">
      <title>Language-based Video Editing via Multi-Modal Multi-Level Transformer</title>
      <author><first>Tsu-Jui</first><last>Fu</last></author>
      <author><first>Xin</first><last>Wang</last></author>
      <author><first>Scott</first><last>Grafton</last></author>
      <author><first>Miguel</first><last>Eckstein</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>45–56</pages>
      <abstract>Video editing tools are widely used nowadays for digital design. Although the demand for these tools is high, the prior knowledge required makes it difficult for novices to get started. Systems that could follow natural language instructions to perform automatic editing would significantly improve accessibility. This paper introduces the language-based video editing (LBVE) task, which allows the model to edit, guided by text instruction, a source video into a target video. LBVE contains two features: 1) the scenario of the source video is preserved instead of generating a completely different video; 2) the semantic is presented differently in the target video, and all changes are controlled by the given instruction. We propose a Multi-Modal Multi-Level Transformer (M<tex-math>^3</tex-math>L-Transformer) to carry out LBVE. The M<tex-math>^3</tex-math>L-Transformer dynamically learns the correspondence between video perception and language semantic at different levels, which benefits both the video understanding and video frame synthesis. We build three new datasets for evaluation, including two diagnostic and one from natural videos with human-labeled text. Extensive experimental results show that M<tex-math>^3</tex-math>L-Transformer is effective for video editing and that LBVE can lead to a new field toward vision-and-language research.</abstract>
      <url hash="1918a5e6">2021.alvr-1.8</url>
    </paper>
    <paper id="9">
      <title>Pathdreamer: A World Model for Indoor Navigation</title>
      <author><first>Jing Yu</first><last>Koh</last></author>
      <author><first>Honglak</first><last>Lee</last></author>
      <author><first>Yinfei</first><last>Yang</last></author>
      <author><first>Jason</first><last>Baldridge</last></author>
      <author><first>Peter</first><last>Anderson</last></author>
      <pages>57–67</pages>
      <abstract>People navigating in unfamiliar buildings take advantage of myriad visual, spatial and semantic cues to efficiently achieve their navigation goals. Towards equipping computational agents with similar capabilities, we introduce Pathdreamer, a visual world model for agents navigating in novel indoor environments. Given one or more previous visual observations, Pathdreamer generates plausible high-resolution 360◦ visual observations (RGB, semantic segmentation and depth) for viewpoints that have not been visited, in buildings not seen during training. In regions of high uncertainty (e.g. predicting around corners, imagining the contents of an unseen room), Pathdreamer can predict diverse scenes, allowing an agent to sample multiple realistic outcomes for a given trajectory. We demonstrate that Pathdreamer encodes useful and accessible visual, spatial and semantic knowledge about human environments by using it in the downstream task of Vision-and-Language Navigation (VLN). Specifically, we show that planning ahead with Pathdreamer brings about half the benefit of looking ahead at actual observations from unobserved parts of the environment. We hope that Pathdreamer will help unlock model-based approaches to challenging embodied navigation tasks such as navigating to specified objects and VLN.</abstract>
      <url hash="de906562">2021.alvr-1.9</url>
    </paper>
  </volume>
</collection>
